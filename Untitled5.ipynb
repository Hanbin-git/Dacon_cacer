{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "11kotd-1NcdWPhNrtykonq-bbUjbRJ54r",
      "authorship_tag": "ABX9TyMLE1/uYm4Zwb1ID4Pi3wt5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/Dacon_cacer/blob/main/Untitled5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67Tx9btmkX7m",
        "outputId": "58b81d79-9489-4ad9-b860-c4bbcf882c78"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip -o \"/content/drive/MyDrive/open_1.zip\" -d \"/content/open_1\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wuaLWGZLg1cb",
        "outputId": "3bc8af13-0bd1-44ca-c706-35daed526f32"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/open_1.zip\n",
            "  inflating: /content/open_1/sample_submission.csv  \n",
            "  inflating: /content/open_1/test.csv  \n",
            "  inflating: /content/open_1/train.csv  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "def get_path(filename):\n",
        "    return \"/content/open_1/\" + filename\n"
      ],
      "metadata": {
        "id": "X47pnDLzhGy6"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# SMOTE 설치\n",
        "!pip install -U imbalanced-learn\n",
        "!pip install lightgbm optuna\n",
        "!pip install -q lightgbm catboost xgboost\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c1_zgPGeiKVN",
        "outputId": "f7dedc3e-64d2-4629-e9a4-4401de40727a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.11/dist-packages (0.13.0)\n",
            "Requirement already satisfied: numpy<3,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.10.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<1,>=0.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (0.1.3)\n",
            "Requirement already satisfied: joblib<2,>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.5.0)\n",
            "Collecting optuna\n",
            "  Downloading optuna-4.4.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from lightgbm) (1.15.3)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.16.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.41)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.14.0)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.3)\n",
            "Downloading optuna-4.4.0-py3-none-any.whl (395 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m395.9/395.9 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.16.2-py3-none-any.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.7/242.7 kB\u001b[0m \u001b[31m26.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.16.2 colorlog-6.9.0 optuna-4.4.0\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.2/99.2 MB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Weighted Voting 앙상블 코드 (with 5-Fold)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# ✅ 경로 함수\n",
        "def get_path(filename):\n",
        "    return \"/content/open_1/\" + filename\n",
        "\n",
        "# ✅ 데이터 로드\n",
        "train = pd.read_csv(get_path(\"train.csv\"))\n",
        "test = pd.read_csv(get_path(\"test.csv\"))\n",
        "submission = pd.read_csv(get_path(\"sample_submission.csv\"))\n",
        "\n",
        "# ✅ Target 분리\n",
        "y = train['Cancer']\n",
        "X = train.drop(columns=['ID', 'Cancer'])\n",
        "X_test = test.drop(columns=['ID'])\n",
        "\n",
        "# ✅ 파생변수 생성 함수\n",
        "def add_derived_features(df):\n",
        "    df = df.copy()\n",
        "    if 'TSH' in df.columns and 'T4' in df.columns:\n",
        "        df['T4_TSH_ratio'] = df['T4'] / (df['TSH'] + 1e-3)\n",
        "    if 'T3' in df.columns and 'Nodule_Size' in df.columns:\n",
        "        df['T3_times_Nodule'] = df['T3'] * df['Nodule_Size']\n",
        "    return df\n",
        "\n",
        "X = add_derived_features(X)\n",
        "X_test = add_derived_features(X_test)\n",
        "\n",
        "# ✅ 전처리 함수\n",
        "def preprocess(df, fit_encoders=None):\n",
        "    df = df.copy()\n",
        "    encoders = fit_encoders if fit_encoders else {}\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].astype(str)\n",
        "            if col not in encoders:\n",
        "                encoders[col] = LabelEncoder().fit(df[col])\n",
        "            df[col] = encoders[col].transform(df[col])\n",
        "        else:\n",
        "            df[col] = SimpleImputer(strategy='mean').fit_transform(df[[col]])\n",
        "    return df, encoders\n",
        "\n",
        "X, encoders = preprocess(X)\n",
        "X_test, _ = preprocess(X_test, encoders)\n",
        "\n",
        "# ✅ 5-Fold Stratified CV + Weighted Voting\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "# ✅ 가중치 설정 (성능 기준 수동 조정 또는 추후 자동화 가능)\n",
        "weights = [2.0, 1.2, 1.0]  # [XGB, LGBM, CAT]\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
        "    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]\n",
        "\n",
        "    model1 = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=fold)\n",
        "    model2 = LGBMClassifier(random_state=fold)\n",
        "    model3 = CatBoostClassifier(verbose=0, random_state=fold)\n",
        "\n",
        "    ensemble_model = VotingClassifier(\n",
        "        estimators=[('xgb', model1), ('lgbm', model2), ('cat', model3)],\n",
        "        voting='soft',\n",
        "        weights=weights\n",
        "    )\n",
        "    ensemble_model.fit(X_tr, y_tr)\n",
        "\n",
        "    oof_preds[valid_idx] = ensemble_model.predict_proba(X_val)[:, 1]\n",
        "    test_preds += ensemble_model.predict_proba(X_test)[:, 1] / kf.n_splits\n",
        "\n",
        "# ✅ Threshold 최적화\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "f1s = [f1_score(y, oof_preds > t) for t in thresholds]\n",
        "best_thresh = thresholds[np.argmax(f1s)]\n",
        "print(f\"📌 Best F1: {max(f1s):.4f} at threshold {best_thresh:.2f}\")\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission['Cancer'] = (test_preds > best_thresh).astype(int)\n",
        "submission.to_csv(\"submission_weighted_voting.csv\", index=False)\n"
      ],
      "metadata": {
        "id": "pmzKE7PQpvMd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ad7199c0-c232-4c4e-b662-8c48642d59f5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:01:47] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010088 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:02:07] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009371 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:02:26] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.014822 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:02:56] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.008952 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:03:31] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010061 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
            "[LightGBM] [Info] Start training from score -1.992343\n",
            "📌 Best F1: 0.4841 at threshold 0.23\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_weighted_voting.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "enCIgvodav-h",
        "outputId": "04bd5adf-a66d-4035-f4bc-a00806469186"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_d49dea4b-8add-4dd7-a8fc-842d00c1766c\", \"submission_weighted_voting.csv\", 600662)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 자동 가중치 조정 기반 VotingClassifier 전체 코드\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.metrics import f1_score\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "\n",
        "# ✅ 데이터 로딩\n",
        "def get_path(filename):\n",
        "    return \"/content/open_1/\" + filename\n",
        "\n",
        "train = pd.read_csv(get_path(\"train.csv\"))\n",
        "test = pd.read_csv(get_path(\"test.csv\"))\n",
        "submission = pd.read_csv(get_path(\"sample_submission.csv\"))\n",
        "\n",
        "y = train['Cancer']\n",
        "X = train.drop(columns=['ID', 'Cancer'])\n",
        "X_test = test.drop(columns=['ID'])\n",
        "\n",
        "# ✅ 파생변수 생성\n",
        "def add_derived_features(df):\n",
        "    df = df.copy()\n",
        "    if 'TSH' in df.columns and 'T4' in df.columns:\n",
        "        df['T4_TSH_ratio'] = df['T4'] / (df['TSH'] + 1e-3)\n",
        "    if 'T3' in df.columns and 'Nodule_Size' in df.columns:\n",
        "        df['T3_times_Nodule'] = df['T3'] * df['Nodule_Size']\n",
        "    return df\n",
        "\n",
        "X = add_derived_features(X)\n",
        "X_test = add_derived_features(X_test)\n",
        "\n",
        "# ✅ 전처리 (문자형: Label Encoding, 수치형: 평균 대체)\n",
        "def preprocess(df, fit_encoders=None):\n",
        "    df = df.copy()\n",
        "    encoders = fit_encoders if fit_encoders else {}\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].astype(str)\n",
        "            if col not in encoders:\n",
        "                encoders[col] = LabelEncoder().fit(df[col])\n",
        "            df[col] = encoders[col].transform(df[col])\n",
        "        else:\n",
        "            df[col] = SimpleImputer(strategy='mean').fit_transform(df[[col]])\n",
        "    return df, encoders\n",
        "\n",
        "X, encoders = preprocess(X)\n",
        "X_test, _ = preprocess(X_test, encoders)\n",
        "\n",
        "# ✅ 개별 모델 F1 평가 함수\n",
        "def get_model_f1(model_cls, X, y, name='model'):\n",
        "    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    oof = np.zeros(len(X))\n",
        "    for train_idx, valid_idx in skf.split(X, y):\n",
        "        model = model_cls()\n",
        "        model.fit(X.iloc[train_idx], y.iloc[train_idx])\n",
        "        oof[valid_idx] = model.predict(X.iloc[valid_idx])\n",
        "    f1 = f1_score(y, oof)\n",
        "    print(f\"✅ {name} F1 Score: {f1:.4f}\")\n",
        "    return f1\n",
        "\n",
        "# ✅ 각 모델별 F1 계산\n",
        "f1_xgb = get_model_f1(lambda: XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=0), X, y, 'XGB')\n",
        "f1_lgb = get_model_f1(lambda: LGBMClassifier(random_state=0), X, y, 'LGBM')\n",
        "f1_cat = get_model_f1(lambda: CatBoostClassifier(verbose=0, random_state=0), X, y, 'CatBoost')\n",
        "\n",
        "# ✅ F1 기준 가중치 설정\n",
        "model_f1s = np.array([f1_xgb, f1_lgb, f1_cat])\n",
        "weights = model_f1s / model_f1s.sum() * 3  # 총합 3 기준 정규화 (VotingClassifier에 넣을 weight)\n",
        "print(\"📌 자동 설정된 weights:\", weights.round(3).tolist())\n",
        "\n",
        "# ✅ VotingClassifier 앙상블 (Soft Voting with weights)\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_preds = np.zeros(len(X))\n",
        "test_preds = np.zeros(len(X_test))\n",
        "\n",
        "for fold, (train_idx, valid_idx) in enumerate(kf.split(X, y)):\n",
        "    X_tr, y_tr = X.iloc[train_idx], y.iloc[train_idx]\n",
        "    X_val, y_val = X.iloc[valid_idx], y.iloc[valid_idx]\n",
        "\n",
        "    model1 = XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=fold)\n",
        "    model2 = LGBMClassifier(random_state=fold)\n",
        "    model3 = CatBoostClassifier(verbose=0, random_state=fold)\n",
        "\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[('xgb', model1), ('lgbm', model2), ('cat', model3)],\n",
        "        voting='soft',\n",
        "        weights=weights.tolist()\n",
        "    )\n",
        "    ensemble.fit(X_tr, y_tr)\n",
        "    oof_preds[valid_idx] = ensemble.predict_proba(X_val)[:, 1]\n",
        "    test_preds += ensemble.predict_proba(X_test)[:, 1] / kf.n_splits\n",
        "\n",
        "# ✅ Threshold 최적화\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "f1s = [f1_score(y, oof_preds > t) for t in thresholds]\n",
        "best_thresh = thresholds[np.argmax(f1s)]\n",
        "print(f\"\\n📊 Best Threshold: {best_thresh:.2f}, Best F1: {max(f1s):.4f}\")\n",
        "\n",
        "# ✅ 제출 파일 생성\n",
        "submission['Cancer'] = (test_preds > best_thresh).astype(int)\n",
        "submission.to_csv(\"submission_weighted_auto.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gx1t-PPXe41O",
        "outputId": "b227c4b8-48ab-41be-b4a9-9ad2f397482d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:03:59] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:04:00] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:04:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:04:01] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n",
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:04:02] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ XGB F1 Score: 0.3098\n",
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015546 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n",
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009474 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n",
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.013714 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n",
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012620 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n",
            "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012904 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
            "[LightGBM] [Info] Start training from score -1.992343\n",
            "✅ LGBM F1 Score: 0.2983\n",
            "✅ CatBoost F1 Score: 0.2921\n",
            "📌 자동 설정된 weights: [1.032, 0.994, 0.973]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:05:34] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010561 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:05:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.010326 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:06:14] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.015175 seconds.\n",
            "You can set `force_col_wise=true` to remove the overhead.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:06:33] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8367, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.012208 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1123\n",
            "[LightGBM] [Info] Number of data points in the train set: 69727, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.119997 -> initscore=-1.992463\n",
            "[LightGBM] [Info] Start training from score -1.992463\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [06:06:54] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[LightGBM] [Info] Number of positive: 8368, number of negative: 61360\n",
            "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.009893 seconds.\n",
            "You can set `force_row_wise=true` to remove the overhead.\n",
            "And if memory is not enough, you can set `force_col_wise=true`.\n",
            "[LightGBM] [Info] Total Bins 1122\n",
            "[LightGBM] [Info] Number of data points in the train set: 69728, number of used features: 14\n",
            "[LightGBM] [Info] [binary:BoostFromScore]: pavg=0.120009 -> initscore=-1.992343\n",
            "[LightGBM] [Info] Start training from score -1.992343\n",
            "\n",
            "📊 Best Threshold: 0.23, Best F1: 0.4854\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_weighted_auto.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "a58jqqMJe91d",
        "outputId": "a68a18aa-b660-4571-b5cc-0a572ce99a58"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_a67f1155-6ebc-476e-87f8-b278d2e37298\", \"submission_weighted_auto.csv\", 600662)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from catboost import CatBoostClassifier\n",
        "\n",
        "# 파일 경로 함수\n",
        "def get_path(filename):\n",
        "    return \"/content/open_1/\" + filename\n",
        "\n",
        "# 데이터 로드\n",
        "train = pd.read_csv(get_path(\"train.csv\"))\n",
        "test = pd.read_csv(get_path(\"test.csv\"))\n",
        "submission = pd.read_csv(get_path(\"sample_submission.csv\"))\n",
        "\n",
        "# Target 구분\n",
        "y = train['Cancer']\n",
        "X = train.drop(columns=['ID', 'Cancer'])\n",
        "X_test = test.drop(columns=['ID'])\n",
        "\n",
        "# 파생변수 생성 함수\n",
        "def add_derived_features(df):\n",
        "    df = df.copy()\n",
        "    num_cols = df.select_dtypes(include='number').columns.tolist()\n",
        "    for col in num_cols:\n",
        "        df[f'{col}_squared'] = df[col] ** 2\n",
        "        df[f'{col}_sqrt'] = np.sqrt(np.abs(df[col]))\n",
        "        df[f'{col}_log'] = np.log1p(np.abs(df[col]))\n",
        "    if 'TSH' in df.columns and 'T4' in df.columns:\n",
        "        df['T4_TSH_ratio'] = df['T4'] / (df['TSH'] + 1e-3)\n",
        "    if 'T3' in df.columns and 'Nodule_Size' in df.columns:\n",
        "        df['T3_times_Nodule'] = df['T3'] * df['Nodule_Size']\n",
        "    return df\n",
        "\n",
        "X = add_derived_features(X)\n",
        "X_test = add_derived_features(X_test)\n",
        "\n",
        "# 전체 전처리\n",
        "def preprocess(df, encoders=None):\n",
        "    df = df.copy()\n",
        "    encoders = encoders or {}\n",
        "    for col in df.columns:\n",
        "        if df[col].dtype == 'object':\n",
        "            df[col] = df[col].astype(str)\n",
        "            if col not in encoders:\n",
        "                encoders[col] = LabelEncoder().fit(df[col])\n",
        "            df[col] = encoders[col].transform(df[col])\n",
        "        else:\n",
        "            df[col] = SimpleImputer(strategy='mean').fit_transform(df[[col]])\n",
        "    return df, encoders\n",
        "\n",
        "X, encoders = preprocess(X)\n",
        "X_test, _ = preprocess(X_test, encoders)\n",
        "\n",
        "# 기본 모델들\n",
        "base_learners = [\n",
        "    ('xgb', XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42)),\n",
        "    ('lgbm', LGBMClassifier(random_state=42)),\n",
        "    ('cat', CatBoostClassifier(verbose=0, random_state=42))\n",
        "]\n",
        "meta_model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "\n",
        "# Stacking 메타 모델\n",
        "stack_model = StackingClassifier(\n",
        "    estimators=base_learners,\n",
        "    final_estimator=meta_model,\n",
        "    cv=5,\n",
        "    passthrough=True,\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "# 학습\n",
        "stack_model.fit(X, y)\n",
        "\n",
        "# 예측\n",
        "test_probs = stack_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Threshold 권절 및 저장\n",
        "thresholds = np.arange(0.1, 0.9, 0.01)\n",
        "oof_preds = stack_model.predict_proba(X)[:, 1]\n",
        "f1s = [f1_score(y, oof_preds > t) for t in thresholds]\n",
        "best_thresh = thresholds[np.argmax(f1s)]\n",
        "print(f\"Best threshold: {best_thresh:.2f}, Best F1: {max(f1s):.4f}\")\n",
        "\n",
        "submission['Cancer'] = (test_probs > best_thresh).astype(int)\n",
        "submission.to_csv(\"submission_stacking.csv\", index=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NepybWE9gE9M",
        "outputId": "8ee0c3d8-942b-4237-bee5-07ece8997430"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best threshold: 0.14, Best F1: 0.5263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"submission_stacking.csv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "zEoxkEQNgI9I",
        "outputId": "26d0e98f-86a9-4eee-9f28-3d4a4c96d42c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6da42cfa-60d1-4bfd-8db8-167fe27355d9\", \"submission_stacking.csv\", 600662)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}
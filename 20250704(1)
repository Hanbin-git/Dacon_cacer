{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.11",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 12337654,
          "sourceType": "datasetVersion",
          "datasetId": 7777606
        }
      ],
      "dockerImageVersionId": 31040,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook90dc24bd09",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hanbin-git/Dacon_cacer/blob/main/20250704(1)\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "rpfN3i6Z4lxH"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "biniroun_drug_data_path = kagglehub.dataset_download('biniroun/drug-data')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "i4zI2JqQ4lxK"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# Kaggle 노트북 상단에 RDKit 설치 명령어 추가\n",
        "!pip install -q rdkit-pypi\n",
        "\n",
        "# ✅ numpy와 scipy의 호환 가능한 버전으로 설치\n",
        "!pip install numpy==1.23.5 scipy==1.10.1 --force-reinstall --no-cache-dir\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T00:25:28.815708Z",
          "iopub.execute_input": "2025-07-04T00:25:28.815993Z",
          "iopub.status.idle": "2025-07-04T00:25:44.735098Z",
          "shell.execute_reply.started": "2025-07-04T00:25:28.815935Z",
          "shell.execute_reply": "2025-07-04T00:25:44.734022Z"
        },
        "id": "VuRdJ4av4lxL",
        "outputId": "8136a243-9b02-4c8b-9e8a-721f08734e42"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Collecting numpy==1.23.5\n  Downloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\nCollecting scipy==1.10.1\n  Downloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (58 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.9/58.9 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading numpy-1.23.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.1/17.1 MB\u001b[0m \u001b[31m260.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading scipy-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.1/34.1 MB\u001b[0m \u001b[31m214.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy, scipy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n  Attempting uninstall: scipy\n    Found existing installation: scipy 1.10.1\n    Uninstalling scipy-1.10.1:\n      Successfully uninstalled scipy-1.10.1\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nmkl-umath 0.1.1 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-random 1.2.4 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\nmkl-fft 1.3.8 requires numpy<1.27.0,>=1.26.4, but you have numpy 1.23.5 which is incompatible.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\nwoodwork 0.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\nfeaturetools 1.31.0 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\npyldavis 3.4.1 requires numpy>=1.24.2, but you have numpy 1.23.5 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.23.5 which is incompatible.\nbayesian-optimization 2.0.3 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\njax 0.5.2 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\npymc 5.21.2 requires numpy>=1.25.0, but you have numpy 1.23.5 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nscikit-image 0.25.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\ntreescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires numpy>=1.24.0, but you have numpy 1.23.5 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\nimbalanced-learn 0.13.0 requires numpy<3,>=1.24.3, but you have numpy 1.23.5 which is incompatible.\nblosc2 3.2.1 requires numpy>=1.26, but you have numpy 1.23.5 which is incompatible.\ncvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\nchex 0.1.89 requires numpy>=1.24.1, but you have numpy 1.23.5 which is incompatible.\ntensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.23.5 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\nalbumentations 2.0.5 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\nxarray 2025.1.2 requires numpy>=1.24, but you have numpy 1.23.5 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\nalbucore 0.0.23 requires numpy>=1.24.4, but you have numpy 1.23.5 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.23.5 scipy-1.10.1\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# 필요한 경우 다른 라이브러리도 함께 업그레이드/설치\n",
        "!pip install --upgrade scikit-learn pandas numpy xgboost lightgbm catboost\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T00:26:25.788415Z",
          "iopub.execute_input": "2025-07-04T00:26:25.788784Z",
          "iopub.status.idle": "2025-07-04T00:26:34.491289Z",
          "shell.execute_reply.started": "2025-07-04T00:26:25.788756Z",
          "shell.execute_reply": "2025-07-04T00:26:34.490305Z"
        },
        "id": "5H1o3bBj4lxN",
        "outputId": "57acb4de-32a9-44b5-a686-739aff9ed2e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.7.0)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (1.23.5)\nCollecting numpy\n  Using cached numpy-2.3.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (62 kB)\nRequirement already satisfied: xgboost in /usr/local/lib/python3.11/dist-packages (3.0.2)\nRequirement already satisfied: lightgbm in /usr/local/lib/python3.11/dist-packages (4.6.0)\nRequirement already satisfied: catboost in /usr/local/lib/python3.11/dist-packages (1.2.8)\nRequirement already satisfied: scipy>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.10.1)\nRequirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.11/dist-packages (from xgboost) (2.21.5)\nRequirement already satisfied: graphviz in /usr/local/lib/python3.11/dist-packages (from catboost) (0.20.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from catboost) (3.7.2)\nRequirement already satisfied: plotly in /usr/local/lib/python3.11/dist-packages (from catboost) (5.24.1)\nRequirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from catboost) (1.17.0)\n  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (1.4.8)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (25.0)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->catboost) (3.0.9)\nRequirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from plotly->catboost) (9.1.2)\nDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m78.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: numpy\n  Attempting uninstall: numpy\n    Found existing installation: numpy 1.23.5\n    Uninstalling numpy-1.23.5:\n      Successfully uninstalled numpy-1.23.5\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\ndask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ncudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\ndatasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.3.2 which is incompatible.\ntsfresh 0.21.0 requires scipy>=1.14.0; python_version >= \"3.10\", but you have scipy 1.10.1 which is incompatible.\nkaggle-environments 1.16.11 requires scipy>=1.11.2, but you have scipy 1.10.1 which is incompatible.\ncategory-encoders 2.7.0 requires scikit-learn<1.6.0,>=1.0.0, but you have scikit-learn 1.7.0 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngoogle-colab 1.0.0 requires google-auth==2.38.0, but you have google-auth 2.40.1 which is incompatible.\ngoogle-colab 1.0.0 requires notebook==6.5.7, but you have notebook 6.5.4 which is incompatible.\ngoogle-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\njax 0.5.2 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\ndopamine-rl 4.1.2 requires gymnasium>=1.0.0, but you have gymnasium 0.29.0 which is incompatible.\nsklearn-compat 0.1.3 requires scikit-learn<1.7,>=1.2, but you have scikit-learn 1.7.0 which is incompatible.\nscikit-image 0.25.2 requires scipy>=1.11.4, but you have scipy 1.10.1 which is incompatible.\nbigframes 1.42.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\ncvxpy 1.6.4 requires scipy>=1.11.0, but you have scipy 1.10.1 which is incompatible.\nplotnine 0.14.5 requires matplotlib>=3.8.0, but you have matplotlib 3.7.2 which is incompatible.\njaxlib 0.5.1 requires scipy>=1.11.1, but you have scipy 1.10.1 which is incompatible.\npandas-gbq 0.28.0 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed numpy-1.26.4\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.kill(os.getpid(), 9)\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "execution_failed": "2025-07-04T00:26:53.664Z"
        },
        "id": "dHzoMXsk4lxO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import matplotlib.pyplot as plt\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Draw\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from collections import Counter\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from scipy.stats import pearsonr"
      ],
      "metadata": {
        "trusted": true,
        "id": "JKXQuRKZ4lxO"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 학습 데이터 불러오기\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# df = pd.read_csv(os.path.join(path, 'train.csv'))"
      ],
      "metadata": {
        "trusted": true,
        "id": "keZZFndl4lxP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# smiles = df['Canonical_Smiles'][1] # idx\n",
        "\n",
        "# # SMILES string을 RDKit molecule object 변환\n",
        "# mol = Chem.MolFromSmiles(smiles)\n",
        "\n",
        "# # 변환이 잘 되었는지 확인\n",
        "# if mol is not None:\n",
        "#     # 분자 구조 이미지 파일로 그리기\n",
        "#     img = Draw.MolToImage(mol)\n",
        "#     # 2D 분자 구조 이미지 저장\n",
        "#     img.save(\"molecule.png\")\n",
        "# else:\n",
        "#     print(\"Invalid SMILES string\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "vuN8Oa3X4lxP"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 각 SMILES에서 원자 정보 추출\n",
        "# element_counter = Counter()\n",
        "# atom_counts = []\n",
        "# invalid_smiles = []\n",
        "# mol_weights = []\n",
        "\n",
        "# for idx, smi in enumerate(df['Canonical_Smiles']):\n",
        "#     mol = Chem.MolFromSmiles(smi)\n",
        "#     if mol is None:\n",
        "#         invalid_smiles.append((idx, smi))\n",
        "#         continue\n",
        "#     atoms = [atom.GetSymbol() for atom in mol.GetAtoms()]\n",
        "#     mol_weight = Descriptors.MolWt(mol)\n",
        "#     element_counter.update(atoms)\n",
        "#     atom_counts.append(len(atoms))\n",
        "#     mol_weights.append(mol_weight)"
      ],
      "metadata": {
        "trusted": true,
        "id": "dLxMTqVp4lxQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 평균 원자 수 (H 제외(heavy atoms), SMILES에만 포함된 원자)\n",
        "# df['atom_count'] = atom_counts\n",
        "# df['mol_weight'] = mol_weights\n",
        "\n",
        "# mean_atoms = np.mean([c for c in atom_counts if c is not None])"
      ],
      "metadata": {
        "trusted": true,
        "id": "P0veaS2i4lxQ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 원소 등장 빈도 (H 제외)\n",
        "# # plt.figure(figsize=(12, 6))\n",
        "# fig, ax = plt.subplots(figsize=(12, 6))\n",
        "# elements, counts = zip(*element_counter.most_common())\n",
        "# bars = ax.bar(elements, counts, color='skyblue')\n",
        "# plt.title('Atom Frequency Distribution')\n",
        "# plt.xlabel('Atom')\n",
        "# plt.ylabel('Frequency')\n",
        "# plt.grid(True, axis='y', linestyle='--', alpha=0.5)\n",
        "\n",
        "# for bar in bars:\n",
        "#     height = bar.get_height()\n",
        "#     plt.text(bar.get_x() + bar.get_width()/2, height,\n",
        "#              f'{height}', ha='center', va='bottom', fontsize=9)\n",
        "\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "24GJqKeE4lxR"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 화합물당 원자 수 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.hist(atom_counts, bins=20, color='orange', edgecolor='black')\n",
        "# plt.title('Distribution of heavy atoms per compound')\n",
        "# plt.xlabel('Number of heavy atoms')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "H8LqQxlq4lxS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # Inhibition 값 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# # 5% 단위로 분리\n",
        "# plt.hist(df['Inhibition'], bins=20, color='green', edgecolor='black')\n",
        "# plt.title('Target Distribution')\n",
        "# plt.xlabel('Inhibition ')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "_WP_1Dba4lxS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 분자량 히스토그램\n",
        "# plt.figure(figsize=(8, 5))\n",
        "# plt.hist(df['mol_weight'], bins=20, color='red', edgecolor='black')\n",
        "# plt.title('Mol Weight Distribution')\n",
        "# plt.xlabel('mol_weight')\n",
        "# plt.ylabel('Number of compound')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "YNq6csz24lxS"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 통계 정보\n",
        "# print(f\"Number of valid SMILES: {len(atom_counts)} / {len(df)}\")\n",
        "# print(f\"Number of Invalid SMILES: {len(invalid_smiles)}\")\n",
        "# print(f\"Average number of heavy atoms: {mean_atoms:.2f}\")"
      ],
      "metadata": {
        "trusted": true,
        "id": "_jAUl4G74lxT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 데이터프레임: df['smiles'], df['target'] 가정\n",
        "# df['smiles_length'] = df['Canonical_Smiles'].apply(len)\n",
        "\n",
        "# # 상관계수 계산\n",
        "# corr, p_value = pearsonr(df['smiles_length'], df['Inhibition'])\n",
        "\n",
        "# print(f\" Pearson 상관계수: {corr:.3f}\")\n",
        "# print(f\" p-value: {p_value:.3e}\")\n",
        "# print(\"=\"*120)\n",
        "\n",
        "# # 산점도 시각화\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(df['smiles_length'], df['Inhibition'], alpha=0.6, color='teal', edgecolors='k')\n",
        "# plt.title('SMILES Length vs Inhibition')\n",
        "# plt.xlabel('SMILES Length')\n",
        "# plt.ylabel('Inhibition')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "hLWwL-3J4lxT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 상관계수 계산\n",
        "# corr, p_value = pearsonr(df['mol_weight'], df['Inhibition'])\n",
        "\n",
        "# print(f\" Pearson 상관계수: {corr:.3f}\")\n",
        "# print(f\" p-value: {p_value:.3e}\")\n",
        "# print(\"=\"*120)\n",
        "\n",
        "# # 산점도 시각화\n",
        "# plt.figure(figsize=(8, 6))\n",
        "# plt.scatter(df['mol_weight'], df['Inhibition'], alpha=0.6, color='steelblue', edgecolors='k')\n",
        "# plt.title('Mol Weight vs Inhibition')\n",
        "# plt.xlabel('Mol Weight')\n",
        "# plt.ylabel('Inhibition')\n",
        "# plt.grid(True, linestyle='--', alpha=0.5)\n",
        "# plt.tight_layout()\n",
        "# plt.show()"
      ],
      "metadata": {
        "trusted": true,
        "id": "GYUv4G3m4lxT"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# print(df.columns.tolist())\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "OPzN4ssA4lxU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "\n",
        "# # ✅ RDKit 파생변수 생성 함수\n",
        "# def featurize(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol is None:\n",
        "#         return np.nan, np.nan\n",
        "#     mol_wt = Descriptors.MolWt(mol)\n",
        "#     tpsa = Descriptors.TPSA(mol)\n",
        "#     return mol_wt, tpsa\n",
        "\n",
        "# train[['MolWt', 'TPSA']] = train['Canonical_Smiles'].apply(\n",
        "#     lambda x: pd.Series(featurize(x))\n",
        "# )\n",
        "\n",
        "# # ✅ 파생 변수 생성\n",
        "# train['smiles_length'] = train['Canonical_Smiles'].apply(len)\n",
        "# train['is_heavy_mol'] = (train['MolWt'] > 500).astype(int)\n",
        "# train['is_long_smiles'] = (train['smiles_length'] > 60).astype(int)\n",
        "# train['is_low_inhibition'] = (train['Inhibition'] < 10).astype(int)\n",
        "# train['log_inhibition'] = np.log1p(train['Inhibition'])\n",
        "\n",
        "# # ✅ Feature / Target 설정\n",
        "# features = ['MolWt', 'TPSA', 'smiles_length', 'is_heavy_mol', 'is_long_smiles', 'is_low_inhibition']\n",
        "# X = train[features]\n",
        "# y = train['log_inhibition']\n",
        "\n",
        "# # ✅ KFold + Quantile Loss\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_preds = np.zeros(len(train))\n",
        "\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
        "#     X_val, y_val = X.iloc[val_idx], y.iloc[val_idx]\n",
        "\n",
        "#     model = LGBMRegressor(\n",
        "#         objective='quantile',\n",
        "#         alpha=0.5,\n",
        "#         n_estimators=200,\n",
        "#         learning_rate=0.05,\n",
        "#         random_state=42\n",
        "#     )\n",
        "#     model.fit(X_train, y_train)\n",
        "#     oof_preds[val_idx] = model.predict(X_val)\n",
        "\n",
        "# # ✅ 평가 (역변환)\n",
        "# rmse = np.sqrt(mean_squared_error(np.expm1(y), np.expm1(oof_preds)))\n",
        "# print(f\"✅ RMSE (Quantile Loss + log1p 역변환): {rmse:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "WU5Ha9Yf4lxU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_squared_error\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# # ✅ 데이터 로드\n",
        "# path = '/kaggle/input/drug-data'  # Colab이면 로컬 경로로 수정\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "\n",
        "# # ✅ 파생변수 생성\n",
        "# train['smiles_length'] = train['Canonical_Smiles'].apply(len)\n",
        "# train['is_long_smiles'] = (train['smiles_length'] > 60).astype(int)\n",
        "# train['is_low_inhibition'] = (train['Inhibition'] < 10).astype(int)\n",
        "# train['is_high_inhibition'] = (train['Inhibition'] > 90).astype(int)\n",
        "\n",
        "# # ✅ log1p 타깃 변환\n",
        "# train['log_inhibition'] = np.log1p(train['Inhibition'])\n",
        "\n",
        "# # ✅ 입력 특성 선택\n",
        "# features = ['smiles_length', 'is_long_smiles', 'is_low_inhibition', 'is_high_inhibition']\n",
        "# X = train[features]\n",
        "# y = train['log_inhibition']\n",
        "\n",
        "# # ✅ KFold 교차검증\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_preds = np.zeros(len(train))\n",
        "\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n",
        "#     y_tr, y_val = y.iloc[tr_idx], y.iloc[val_idx]\n",
        "\n",
        "#     train_pool = Pool(X_tr, y_tr)\n",
        "#     val_pool = Pool(X_val, y_val)\n",
        "\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=1000,\n",
        "#         learning_rate=0.05,\n",
        "#         depth=6,\n",
        "#         loss_function='Quantile:alpha=0.5',  # Median\n",
        "#         random_seed=42,\n",
        "#         verbose=0\n",
        "#     )\n",
        "\n",
        "#     model.fit(train_pool, eval_set=val_pool, early_stopping_rounds=50)\n",
        "#     oof_preds[val_idx] = model.predict(X_val)\n",
        "\n",
        "# # ✅ 평가 (log1p 역변환 후 RMSE)\n",
        "# rmse = np.sqrt(mean_squared_error(np.expm1(y), np.expm1(oof_preds)))\n",
        "# print(f\"✅ CatBoost RMSE (exp scale): {rmse:.5f}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "67nGk_p94lxU"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # test.csv 로드 및 동일한 파생변수 생성\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# test['smiles_length'] = test['Canonical_Smiles'].apply(len)\n",
        "# test['is_long_smiles'] = (test['smiles_length'] > 60).astype(int)\n",
        "# test['is_low_inhibition'] = 0  # 예측 대상이므로 실제 값 없음\n",
        "# test['is_high_inhibition'] = 0\n",
        "\n",
        "# test_X = test[features]\n",
        "\n",
        "# # 5개 모델 평균 앙상블\n",
        "# preds = np.zeros(len(test_X))\n",
        "# for fold, (tr_idx, val_idx) in enumerate(kf.split(X)):\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=1000,\n",
        "#         learning_rate=0.05,\n",
        "#         depth=6,\n",
        "#         loss_function='Quantile:alpha=0.5',\n",
        "#         random_seed=42,\n",
        "#         verbose=0\n",
        "#     )\n",
        "#     model.fit(X.iloc[tr_idx], y.iloc[tr_idx])\n",
        "#     preds += model.predict(test_X) / kf.get_n_splits()\n",
        "\n",
        "# # log1p 역변환 후 제출\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "# submission['Inhibition'] = np.expm1(preds)\n",
        "# submission.to_csv(\"submission_catboost.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "NtULBXwz4lxV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 확실한 예측 및 저장 코드\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "# submission['Inhibition'] = np.expm1(preds)  # 로그 역변환 필수\n",
        "# print(submission['Inhibition'].describe())  # 분포 확인\n",
        "\n",
        "# submission.to_csv(\"submission_catboost_fixed.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "61DLo79v4lxV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# plt.hist(submission['Inhibition'], bins=30, color='skyblue', edgecolor='black')\n",
        "# plt.title(\"Distribution of Predicted Inhibition Values\")\n",
        "# plt.xlabel(\"Inhibition\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "6dF70ObN4lxV"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "\n",
        "# # ✅ SMILES → RDKit mol 객체로 변환\n",
        "# def smiles_to_mol(smiles):\n",
        "#     try:\n",
        "#         return Chem.MolFromSmiles(smiles)\n",
        "#     except:\n",
        "#         return None\n",
        "\n",
        "# train[\"mol\"] = train[\"Canonical_Smiles\"].apply(smiles_to_mol)\n",
        "# test[\"mol\"] = test[\"Canonical_Smiles\"].apply(smiles_to_mol)\n",
        "\n",
        "# # ✅ 파생변수 생성\n",
        "# def make_features(df, is_train=True):\n",
        "#     df[\"mol_wt\"] = df[\"mol\"].apply(lambda m: Descriptors.MolWt(m) if m else 0)\n",
        "#     df[\"smiles_len\"] = df[\"Canonical_Smiles\"].apply(len)\n",
        "#     df[\"is_heavy_mol\"] = (df[\"mol_wt\"] > 500).astype(int)\n",
        "#     df[\"long_smiles\"] = (df[\"smiles_len\"] > 70).astype(int)\n",
        "#     if is_train:\n",
        "#         df[\"low_inhibition\"] = (df[\"Inhibition\"] < 30).astype(int)\n",
        "#     return df\n",
        "\n",
        "# train = make_features(train, is_train=True)\n",
        "# test = make_features(test, is_train=False)\n",
        "\n",
        "# # ✅ Target log1p 변환\n",
        "# train[\"target\"] = np.log1p(train[\"Inhibition\"])\n",
        "\n",
        "# # ✅ 모델 학습\n",
        "# features = [\"mol_wt\", \"smiles_len\", \"is_heavy_mol\", \"long_smiles\"]\n",
        "# X = train[features]\n",
        "# y = train[\"target\"]\n",
        "# X_test = test[features]\n",
        "\n",
        "# X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# model = CatBoostRegressor(\n",
        "#     iterations=1000,\n",
        "#     learning_rate=0.03,\n",
        "#     depth=6,\n",
        "#     loss_function=\"Quantile:alpha=0.5\",  # Quantile Regression\n",
        "#     early_stopping_rounds=30,\n",
        "#     random_seed=42,\n",
        "#     verbose=100\n",
        "# )\n",
        "\n",
        "# model.fit(Pool(X_train, y_train), eval_set=Pool(X_valid, y_valid))\n",
        "\n",
        "# # ✅ 예측 및 역변환\n",
        "# preds = model.predict(X_test)\n",
        "# submission[\"Inhibition\"] = np.expm1(preds)  # log1p 역변환\n",
        "\n",
        "# # ✅ 저장\n",
        "# submission.to_csv(\"submission_catboost_v2.csv\", index=False)\n",
        "\n",
        "# # ✅ 예측 분포 시각화\n",
        "# plt.figure(figsize=(8, 4))\n",
        "# plt.hist(submission[\"Inhibition\"], bins=30, color=\"skyblue\", edgecolor=\"black\")\n",
        "# plt.title(\"Distribution of Predicted Inhibition Values\")\n",
        "# plt.xlabel(\"Inhibition\")\n",
        "# plt.ylabel(\"Frequency\")\n",
        "# plt.grid(True)\n",
        "# plt.show()\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "eNPZKU4y4lxW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # 라이브러리\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# from sklearn.model_selection import train_test_split\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "\n",
        "\n",
        "# # 로그 변환\n",
        "# train[\"Inhibition_log\"] = np.log1p(train[\"Inhibition\"])\n",
        "\n",
        "# # 서브구조 파생변수 추가 함수\n",
        "# def add_smiles_features(df):\n",
        "#     features = ['Cl', 'Br', 'F', 'N=', 'C#C', 'c1ccccc1']  # 예시\n",
        "#     for f in features:\n",
        "#         df[f\"has_{f}\"] = df[\"Canonical_Smiles\"].str.contains(f, regex=False).astype(int)\n",
        "#     return df\n",
        "\n",
        "# train = add_smiles_features(train)\n",
        "# test = add_smiles_features(test)\n",
        "\n",
        "# # 피처 및 타겟 분리\n",
        "# X = train.drop(columns=[\"ID\", \"Inhibition\", \"Inhibition_log\", \"Canonical_Smiles\"])\n",
        "# y = train[\"Inhibition_log\"]\n",
        "# X_test = test.drop(columns=[\"ID\", \"Canonical_Smiles\"])\n",
        "\n",
        "# # 이상치 가중치\n",
        "# sample_weight = np.where(train[\"Inhibition\"] > 45, 2.0, 1.0)\n",
        "\n",
        "# # train/valid split\n",
        "# X_train, X_valid, y_train, y_valid, sw_train, sw_valid = train_test_split(\n",
        "#     X, y, sample_weight, test_size=0.2, random_state=42\n",
        "# )\n",
        "\n",
        "# # 모델 학습 (alpha=0.9)\n",
        "# model_q90 = CatBoostRegressor(\n",
        "#     loss_function='Quantile:alpha=0.9',\n",
        "#     iterations=500,\n",
        "#     learning_rate=0.05,\n",
        "#     depth=6,\n",
        "#     random_state=42,\n",
        "#     verbose=100\n",
        "# )\n",
        "\n",
        "# model_q90.fit(Pool(X_train, y_train, weight=sw_train),\n",
        "#               eval_set=Pool(X_valid, y_valid, weight=sw_valid))\n",
        "\n",
        "# # 예측 및 복원\n",
        "# preds = model_q90.predict(X_test)\n",
        "# submission = pd.DataFrame({\n",
        "#     \"ID\": test[\"ID\"],\n",
        "#     \"Inhibition\": np.expm1(preds)  # 로그 복원\n",
        "# })\n",
        "\n",
        "# # 저장\n",
        "# submission.to_csv(\"submission_catboost_quantile090.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "1rYje2b34lxW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 라이브러리\n",
        "# import os\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from catboost import CatBoostRegressor\n",
        "# from lightgbm import LGBMRegressor\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ 파생변수 생성 함수\n",
        "# def smiles_to_descriptors(smiles):\n",
        "#     mol = Chem.MolFromSmiles(smiles)\n",
        "#     if mol:\n",
        "#         return {\n",
        "#             'MolWt': Descriptors.MolWt(mol),\n",
        "#             'NumHDonors': Descriptors.NumHDonors(mol),\n",
        "#             'NumHAcceptors': Descriptors.NumHAcceptors(mol),\n",
        "#             'TPSA': Descriptors.TPSA(mol),\n",
        "#             'LogP': Descriptors.MolLogP(mol)\n",
        "#         }\n",
        "#     else:\n",
        "#         return {'MolWt': 0, 'NumHDonors': 0, 'NumHAcceptors': 0, 'TPSA': 0, 'LogP': 0}\n",
        "\n",
        "# # ✅ 파생변수 적용\n",
        "# train_desc = train['Canonical_Smiles'].apply(smiles_to_descriptors).apply(pd.Series)\n",
        "# test_desc = test['Canonical_Smiles'].apply(smiles_to_descriptors).apply(pd.Series)\n",
        "\n",
        "# train = pd.concat([train, train_desc], axis=1)\n",
        "# test = pd.concat([test, test_desc], axis=1)\n",
        "\n",
        "# # ✅ 모델 입력 설정\n",
        "# features = ['MolWt', 'NumHDonors', 'NumHAcceptors', 'TPSA', 'LogP']\n",
        "# X = train[features].values\n",
        "# X_test = test[features].values\n",
        "# y = train['Inhibition'].values\n",
        "\n",
        "# # ✅ 극단값 가중치 부여 (예: 40 이상인 경우 가중치 ↑)\n",
        "# sample_weight = np.where(y >= 40, 2.0, 1.0)\n",
        "\n",
        "# # ✅ OOF 및 테스트 예측 저장용\n",
        "# oof_cat = np.zeros(len(train))\n",
        "# oof_lgb = np.zeros(len(train))\n",
        "# preds_cat = np.zeros(len(test))\n",
        "# preds_lgb = np.zeros(len(test))\n",
        "\n",
        "# # ✅ KFold 설정\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# # ✅ 개별 모델 학습 및 예측\n",
        "# for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n",
        "#     X_tr, X_val = X[train_idx], X[val_idx]\n",
        "#     y_tr, y_val = y[train_idx], y[val_idx]\n",
        "#     w_tr = sample_weight[train_idx]\n",
        "\n",
        "#     # ✅ CatBoost\n",
        "#     cat = CatBoostRegressor(verbose=0, iterations=500, learning_rate=0.05)\n",
        "#     cat.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     preds_cat += cat.predict(X_test) / kf.n_splits\n",
        "\n",
        "#     # ✅ LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=500, learning_rate=0.05)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     preds_lgb += lgb.predict(X_test) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타모델 입력 구성\n",
        "# X_meta = np.vstack([oof_cat, oof_lgb]).T\n",
        "# X_test_meta = np.vstack([preds_cat, preds_lgb]).T\n",
        "\n",
        "# # ✅ 메타모델 (Ridge)\n",
        "# meta_model = Ridge()\n",
        "# meta_model.fit(X_meta, y, sample_weight=sample_weight)\n",
        "# final_preds = meta_model.predict(X_test_meta)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking.csv', index=False)\n",
        "# print(\"✅ 최종 제출 파일 저장 완료.\")\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "UwipVgrj4lxW"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.linear_model import QuantileRegressor\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.model_selection import KFold\n",
        "# import optuna\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ 전처리 완료된 특징 불러오기 (RDKit 기반)\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Optuna Objective 정의\n",
        "# def objective(trial):\n",
        "#     alpha = trial.suggest_float(\"alpha\", 0.7, 0.95)\n",
        "#     kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "#     maes = []\n",
        "\n",
        "#     for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#         X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#         y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "#         model = QuantileRegressor(quantile=alpha, alpha=0, solver='highs')\n",
        "#         model.fit(X_tr, y_tr)\n",
        "#         y_pred = model.predict(X_val)\n",
        "#         maes.append(mean_absolute_error(y_val, y_pred))\n",
        "\n",
        "#     return np.mean(maes)\n",
        "\n",
        "# # ✅ Optuna 실행\n",
        "# study = optuna.create_study(direction='minimize')\n",
        "# study.optimize(objective, n_trials=30)\n",
        "# best_alpha = study.best_params['alpha']\n",
        "\n",
        "# # ✅ 최종 모델 학습\n",
        "# final_model = QuantileRegressor(quantile=best_alpha, alpha=0, solver='highs')\n",
        "# final_model.fit(X_train_scaled, y_train)\n",
        "# preds = final_model.predict(X_test_scaled)\n",
        "\n",
        "# # ✅ 제출 파일 저장\n",
        "# submission['Inhibition'] = preds\n",
        "# submission.to_csv('submission_quantile_optuna.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "mtGgolzt4lxX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출 (이전 단계 코드 활용)\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 스케일링\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Sample Weight 정의 (높은 억제율에 더 높은 가중치 부여 예시)\n",
        "# sample_weight = np.log1p(y_train)  # or y_train**1.5 등\n",
        "\n",
        "# # ✅ 모델 학습 (5-Fold CV)\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# preds = np.zeros(len(X_test))\n",
        "# val_mae_list = []\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#     X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     model = CatBoostRegressor(\n",
        "#         iterations=2000,\n",
        "#         learning_rate=0.03,\n",
        "#         depth=6,\n",
        "#         eval_metric='MAE',\n",
        "#         early_stopping_rounds=100,\n",
        "#         verbose=0,\n",
        "#         random_state=42\n",
        "#     )\n",
        "\n",
        "#     train_pool = Pool(X_tr, y_tr, weight=w_tr)\n",
        "#     val_pool = Pool(X_val, y_val)\n",
        "#     model.fit(train_pool, eval_set=val_pool)\n",
        "\n",
        "#     val_pred = model.predict(X_val)\n",
        "#     val_mae = mean_absolute_error(y_val, val_pred)\n",
        "#     val_mae_list.append(val_mae)\n",
        "\n",
        "#     preds += model.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "# # ✅ 성능 출력 및 제출 파일 저장\n",
        "# print(f\"평균 MAE: {np.mean(val_mae_list):.4f}\")\n",
        "\n",
        "# submission['Inhibition'] = preds\n",
        "# submission.to_csv(\"submission_catboost_weighted.csv\", index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "0p8MusEM4lxX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import LinearRegression\n",
        "# from xgboost import XGBRegressor\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# # ✅ Sample Weight 정의\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking: Base 모델 예측값을 모아 최종 모델 학습\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_scaled):\n",
        "#     X_tr, X_val = X_train_scaled[train_idx], X_train_scaled[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=500, learning_rate=0.03, max_depth=6, random_state=42, verbosity=0)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_scaled) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델 학습 (Linear Regression)\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = LinearRegression()\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_cat_lgb_xgb.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "VOxMos6E4lxX"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.model_selection import KFold\n",
        "# from sklearn.metrics import mean_absolute_error\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from sklearn.linear_model import Ridge\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit 기반 특징 추출\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem.Crippen import MolLogP\n",
        "# from rdkit.Chem.Descriptors import MolWt, NumRotatableBonds\n",
        "# from rdkit.Chem.Lipinski import NumHDonors, NumHAcceptors\n",
        "# from rdkit.Chem.rdMolDescriptors import CalcTPSA\n",
        "\n",
        "# def extract_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_features(train)\n",
        "# X_test = extract_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_train.columns)\n",
        "\n",
        "# # ✅ Sample Weight 정의\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델 학습\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = Ridge(alpha=1.0)\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_final.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "JjIZhv294lxY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# from catboost import CatBoostRegressor, Pool\n",
        "# from lightgbm import LGBMRegressor\n",
        "# from xgboost import XGBRegressor\n",
        "# from sklearn.linear_model import Ridge\n",
        "# from sklearn.model_selection import KFold\n",
        "\n",
        "# # ✅ 데이터 로딩\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ RDKit Feature 확장\n",
        "# def extract_rdkit_features(df):\n",
        "#     mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "#     features = {\n",
        "#         'MolWt': [Descriptors.MolWt(mol) if mol else np.nan for mol in mols],\n",
        "#         'LogP': [Crippen.MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHDonors': [Lipinski.NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumHAcceptors': [Lipinski.NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "#         'TPSA': [rdMolDescriptors.CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumRotatableBonds': [Descriptors.NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "#         'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols],\n",
        "#         'HeavyAtomCount': [mol.GetNumHeavyAtoms() if mol else np.nan for mol in mols],\n",
        "#         'FractionCSP3': [rdMolDescriptors.CalcFractionCSP3(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAliphaticRings': [rdMolDescriptors.CalcNumAliphaticRings(mol) if mol else np.nan for mol in mols],\n",
        "#         'NumAromaticRings': [rdMolDescriptors.CalcNumAromaticRings(mol) if mol else np.nan for mol in mols]\n",
        "#     }\n",
        "#     return pd.DataFrame(features)\n",
        "\n",
        "# X_train = extract_rdkit_features(train)\n",
        "# X_test = extract_rdkit_features(test)\n",
        "# y_train = train[\"Inhibition\"]\n",
        "\n",
        "# # ✅ 정규화\n",
        "# scaler = StandardScaler()\n",
        "# X_train_scaled = scaler.fit_transform(X_train)\n",
        "# X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "# X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# # ✅ Sample Weight\n",
        "# sample_weight = np.log1p(y_train)\n",
        "\n",
        "# # ✅ 수동 Stacking\n",
        "# kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "# oof_cat = np.zeros(len(X_train))\n",
        "# oof_lgb = np.zeros(len(X_train))\n",
        "# oof_xgb = np.zeros(len(X_train))\n",
        "# test_cat = np.zeros(len(X_test))\n",
        "# test_lgb = np.zeros(len(X_test))\n",
        "# test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "# for train_idx, val_idx in kf.split(X_train_df):\n",
        "#     X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "#     y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "#     w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "#     # CatBoost\n",
        "#     cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "#     cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "#     oof_cat[val_idx] = cat.predict(X_val)\n",
        "#     test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # LightGBM\n",
        "#     lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "#     lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "#     test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "#     # XGBoost\n",
        "#     xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "#     xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "#     oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "#     test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# # ✅ 메타 모델\n",
        "# stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "# stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "# meta_model = Ridge(alpha=1.0)\n",
        "# meta_model.fit(stacked_train, y_train)\n",
        "# final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# # ✅ 제출\n",
        "# submission['Inhibition'] = final_preds\n",
        "# submission.to_csv('submission_stacking_rdkit_expanded.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "RrEiVusH4lxY"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# # ✅ 기본 라이브러리 및 데이터 불러오기\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "# import os\n",
        "# from rdkit import Chem\n",
        "# from rdkit.Chem import AllChem\n",
        "# import warnings\n",
        "# warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# # ✅ 경로 설정\n",
        "# path = '/kaggle/input/drug-data'\n",
        "# train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "# test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "# submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# # ✅ Morgan Fingerprint 추출 함수\n",
        "# def get_morgan_fingerprint(smiles_string, n_bits=2048, radius=2):\n",
        "#     mol = Chem.MolFromSmiles(smiles_string)\n",
        "#     if mol is not None:\n",
        "#         fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius, nBits=n_bits)\n",
        "#         return np.array(fp)\n",
        "#     else:\n",
        "#         return np.zeros(n_bits, dtype=int)\n",
        "\n",
        "# # ✅ Fingerprint 생성\n",
        "# train_fps = train['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "# test_fps = test['Canonical_Smiles'].apply(get_morgan_fingerprint)\n",
        "\n",
        "# fp_columns = [f'FP_{i}' for i in range(2048)]\n",
        "# X_train = pd.DataFrame(train_fps.tolist(), columns=fp_columns)\n",
        "# X_test = pd.DataFrame(test_fps.tolist(), columns=fp_columns)\n",
        "# X_train['Inhibition'] = train['Inhibition']\n",
        "\n",
        "# # ✅ PyCaret 설치 (Kaggle 환경용)\n",
        "# !pip install -q pycaret\n",
        "\n",
        "# # ✅ PyCaret 설정\n",
        "# from pycaret.regression import *\n",
        "# s = setup(data=X_train, target='Inhibition', session_id=42, normalize=True, normalize_method='zscore', silent=True, verbose=False)\n",
        "\n",
        "# # ✅ 트리 기반 모델만 비교\n",
        "# best = compare_models(include=['lightgbm', 'xgboost', 'catboost', 'rf'], sort='RMSE', n_select=1)\n",
        "\n",
        "# # ✅ 모델 튜닝\n",
        "# tuned = tune_model(best, optimize='RMSE')\n",
        "\n",
        "# # ✅ 모델 앙상블 (스태킹)\n",
        "# top3 = compare_models(include=['lightgbm', 'xgboost', 'catboost'], n_select=3)\n",
        "# blended = blend_models(estimator_list=top3)\n",
        "# final_model = finalize_model(blended)\n",
        "\n",
        "# # ✅ 예측\n",
        "# preds = predict_model(final_model, data=X_test)\n",
        "\n",
        "# # ✅ 제출파일 저장\n",
        "# submission['Inhibition'] = np.clip(preds['prediction_label'], 0, 100)\n",
        "# submission.to_csv('submission_pycaret_stack.csv', index=False)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "yGaDXoSD4lxZ"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "from rdkit import Chem\n",
        "from rdkit.Chem import Descriptors, Crippen, Lipinski, rdMolDescriptors, AllChem\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from catboost import CatBoostRegressor, Pool\n",
        "from lightgbm import LGBMRegressor\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "# ✅ 데이터 로딩\n",
        "path = '/kaggle/input/drug-data'\n",
        "train = pd.read_csv(os.path.join(path, 'train.csv'))\n",
        "test = pd.read_csv(os.path.join(path, 'test.csv'))\n",
        "submission = pd.read_csv(os.path.join(path, 'sample_submission.csv'))\n",
        "\n",
        "# ✅ 1. RDKit 화학 특성 추출\n",
        "def extract_rdkit_features(df):\n",
        "    mols = [Chem.MolFromSmiles(smi) for smi in df['Canonical_Smiles']]\n",
        "    features = {\n",
        "        'MolWt': [Descriptors.MolWt(mol) if mol else np.nan for mol in mols],\n",
        "        'LogP': [Crippen.MolLogP(mol) if mol else np.nan for mol in mols],\n",
        "        'NumHDonors': [Lipinski.NumHDonors(mol) if mol else np.nan for mol in mols],\n",
        "        'NumHAcceptors': [Lipinski.NumHAcceptors(mol) if mol else np.nan for mol in mols],\n",
        "        'TPSA': [rdMolDescriptors.CalcTPSA(mol) if mol else np.nan for mol in mols],\n",
        "        'NumRotatableBonds': [Descriptors.NumRotatableBonds(mol) if mol else np.nan for mol in mols],\n",
        "        'RingCount': [mol.GetRingInfo().NumRings() if mol else np.nan for mol in mols],\n",
        "        'HeavyAtomCount': [mol.GetNumHeavyAtoms() if mol else np.nan for mol in mols],\n",
        "        'FractionCSP3': [rdMolDescriptors.CalcFractionCSP3(mol) if mol else np.nan for mol in mols],\n",
        "        'NumAliphaticRings': [rdMolDescriptors.CalcNumAliphaticRings(mol) if mol else np.nan for mol in mols],\n",
        "        'NumAromaticRings': [rdMolDescriptors.CalcNumAromaticRings(mol) if mol else np.nan for mol in mols]\n",
        "    }\n",
        "    return pd.DataFrame(features)\n",
        "\n",
        "# ✅ 2. Morgan Fingerprint (2048-bit)\n",
        "def get_morgan_fingerprint(smiles, radius=2, nBits=2048):\n",
        "    mol = Chem.MolFromSmiles(smiles)\n",
        "    if mol:\n",
        "        fp = AllChem.GetMorganFingerprintAsBitVect(mol, radius=radius, nBits=nBits)\n",
        "        return np.array(fp)\n",
        "    else:\n",
        "        return np.zeros(nBits)\n",
        "\n",
        "def extract_morgan_df(df, nBits=2048):\n",
        "    fps = df['Canonical_Smiles'].apply(lambda x: get_morgan_fingerprint(x, nBits=nBits))\n",
        "    return pd.DataFrame(fps.tolist(), columns=[f'MFP_{i}' for i in range(nBits)])\n",
        "\n",
        "# ✅ 3. Feature 결합\n",
        "X_train_rdkit = extract_rdkit_features(train)\n",
        "X_test_rdkit = extract_rdkit_features(test)\n",
        "X_train_morgan = extract_morgan_df(train)\n",
        "X_test_morgan = extract_morgan_df(test)\n",
        "\n",
        "# 결합 (RDKit + Morgan)\n",
        "X_train = pd.concat([X_train_rdkit, X_train_morgan], axis=1)\n",
        "X_test = pd.concat([X_test_rdkit, X_test_morgan], axis=1)\n",
        "y_train = train[\"Inhibition\"]\n",
        "\n",
        "# ✅ 4. 정규화\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "X_train_df = pd.DataFrame(X_train_scaled, columns=X_train.columns)\n",
        "X_test_df = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
        "\n",
        "# ✅ 5. Sample Weighting\n",
        "sample_weight = np.log1p(y_train)\n",
        "\n",
        "# ✅ 6. Stacking 앙상블\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "oof_cat = np.zeros(len(X_train))\n",
        "oof_lgb = np.zeros(len(X_train))\n",
        "oof_xgb = np.zeros(len(X_train))\n",
        "test_cat = np.zeros(len(X_test))\n",
        "test_lgb = np.zeros(len(X_test))\n",
        "test_xgb = np.zeros(len(X_test))\n",
        "\n",
        "for train_idx, val_idx in kf.split(X_train_df):\n",
        "    X_tr, X_val = X_train_df.iloc[train_idx], X_train_df.iloc[val_idx]\n",
        "    y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "    w_tr = sample_weight.iloc[train_idx]\n",
        "\n",
        "    # CatBoost\n",
        "    cat = CatBoostRegressor(iterations=1500, learning_rate=0.03, depth=6, verbose=0, early_stopping_rounds=100, random_state=42)\n",
        "    cat.fit(Pool(X_tr, y_tr, weight=w_tr), eval_set=Pool(X_val, y_val))\n",
        "    oof_cat[val_idx] = cat.predict(X_val)\n",
        "    test_cat += cat.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "    # LightGBM\n",
        "    lgb = LGBMRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42, verbose=-1)\n",
        "    lgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "    oof_lgb[val_idx] = lgb.predict(X_val)\n",
        "    test_lgb += lgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "    # XGBoost\n",
        "    xgb = XGBRegressor(n_estimators=1500, learning_rate=0.03, max_depth=6, random_state=42)\n",
        "    xgb.fit(X_tr, y_tr, sample_weight=w_tr)\n",
        "    oof_xgb[val_idx] = xgb.predict(X_val)\n",
        "    test_xgb += xgb.predict(X_test_df) / kf.n_splits\n",
        "\n",
        "# ✅ 7. 메타 모델\n",
        "stacked_train = np.vstack([oof_cat, oof_lgb, oof_xgb]).T\n",
        "stacked_test = np.vstack([test_cat, test_lgb, test_xgb]).T\n",
        "\n",
        "meta_model = Ridge(alpha=1.0)\n",
        "meta_model.fit(stacked_train, y_train)\n",
        "final_preds = meta_model.predict(stacked_test)\n",
        "\n",
        "# ✅ 8. 제출\n",
        "submission['Inhibition'] = final_preds\n",
        "submission.to_csv('submission_stacking_rdkit_morgan.csv', index=False)\n",
        "print(\"✅ 'submission_stacking_rdkit_morgan.csv' 생성 완료!\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-07-04T00:27:05.259111Z",
          "iopub.execute_input": "2025-07-04T00:27:05.25948Z",
          "iopub.status.idle": "2025-07-04T00:29:30.706998Z",
          "shell.execute_reply.started": "2025-07-04T00:27:05.259454Z",
          "shell.execute_reply": "2025-07-04T00:29:30.706025Z"
        },
        "id": "v3Vanrqz4lxZ",
        "outputId": "4773586e-1eea-428c-eddc-15cdc41e3962"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "✅ 'submission_stacking_rdkit_morgan.csv' 생성 완료!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    }
  ]
}